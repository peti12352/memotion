# Meme Emotion Recognition System (v2.1)

## Project Overview

This is a multimodal deep learning system that recognizes emotions in internet memes by analyzing both visual content and text components. The system combines state-of-the-art vision models (CLIP) with text encoders (RoBERTa) through a custom cross-modal attention mechanism.

## Key Components

- **Data Processing**: Custom dataset class for the Memotion dataset with image and text preprocessing
- **Neural Architecture**: Cross-modal fusion between CLIP and RoBERTa embeddings
- **Training Pipeline**: Multi-label classification with custom loss functions
- **API Deployment**: FastAPI server with OCR capabilities for real-time inference

## Technologies

- PyTorch for deep learning models
- Transformers library (Hugging Face) for pretrained models
- OpenCV and Tesseract for OCR
- FastAPI for API deployment
- scikit-learn for evaluation metrics

## Project Structure

- `project_base.mdx`: Main implementation file containing all components
- `data/memotion/`: Dataset directory with images and label files
- Training output: `meme_emotion_model.pth`

## Implementation Details

- Uses CLIP vision model for processing meme images
- Uses RoBERTa for processing text extracted from memes
- Custom cross-attention mechanism for multimodal fusion
- Trained on 5 emotion classes: amusement, sarcasm, offense, motivation, neutral
- Model is deployed as a REST API accepting image uploads

## Usage

- Training: `python meme_emotion.py --train`
- API Deployment: `python meme_emotion.py --api`
- Inference: POST request with meme image to `/predict` endpoint

## Future Improvements

- Add data augmentation for improved generalization
- Implement more complex fusion mechanisms
- Optimize for mobile deployment
- Add explainability features for model predictions
